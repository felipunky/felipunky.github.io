<html>
<head>
    <title>Optimization</title>
    <style>

        body {
            margin: 0;
            background-color: black;
            color: white;
        }

        .First {
            width: auto;
            height: 100%;
        }

        img {
            width: 100%;
            height: auto;
        }
    </style>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
        }
        });
    </script>
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
</head>
<body>
    <h1>
        Optimization
    </h1>
    <div class="First">
        <p>
            According to the <a href="https://www.merriam-webster.com/dictionary/optimization">Merriam-Webster dictionary</a> optimization is “an act, process, or methodology of making something (such as a design, system, or decision) as fully perfect, functional, or effective as possible
            <br>
            specifically: the mathematical procedures (such as finding the maximum of a function) involved in this”
        </p>
        <h1>
            Analytical Solutions
        </h1>
        <p>
            Mathematically (using differential calculus) we could solve an <a href="https://www.youtube.com/watch?v=Ef22yTJDUZI">easy problem</a> such as:
            Two numbers whose product $xy=-16$, and the sum of their squares $S(x)=x^2+y^2$ is at it's $Minimum$.
            We want to solve for one variable
            $$y=\frac{-16}{x}$$
            If we substitute $y$ into $S(x)$:
            $$S(x)=x^2+\frac{-16}{x}^2$$
            $$S(x)=x^2+\frac{256}{x^2}$$
            A function is at it's local $Maximum$ or $Minimum$ point when it's derivative $S^{\prime}(x)=0$
            $$S^{\prime}(x)=2x+(-2)(256)^{-3}=0$$
            $$S^{\prime}(x)=2x-512^{-3}=0$$
            $$S^{\prime}(x)=2x=512^{-3}$$
            $$S^{\prime}(x)=2x^4=512$$
            $$S^{\prime}(x)=x^4=256$$
            If we use the natural logarithm $\ln$ to solve for $x$
            $$\frac{\ln(256)}{\ln(4)}\pm4$$
            $$y=-4\lor{y=4}$$
            To get $x$
            $$y=\frac{-16}{x}$$
            $$xy=-16$$
            $$x=\frac{-16}{y}$$
            $$x=\frac{-16}{-4}=4\lor{x=\frac{-16}{4}=-4}$$
            To prove we are at a $Minimum$ we compute the second derivate of $S^{\prime\prime}(x)$ to check if we are concave upwards $\cup$ or concave downwards $\cap$
            $$S^{\prime\prime}(x)=2+1536x^{-4}$$
            $$S^{\prime\prime}(4)=2+1536\cdot\frac{1}{256}$$
            $$S^{\prime\prime}(4)=8$$
            $8$ is a positive number, so $x=4$ is positive upwards $S^{\prime\prime}(4)=\cup$ meaning we are at a $Minimum$
            Why does this work? The derivative $f^{\prime}(x)$ of a function gives us the slope/tangent, that means that when the function's slope is at $0$, we are at a local minimum or maximum.
        </p>
        <img src="felipunky.github.io/Images/Optimization/x2y2.png" alt="x2y2">
        <p>
            In the example above, we are only dealing with one variable $x$ (although we are solving a two variable problem we are substituting to only solve for one variable), but what happens if we have a function that we want to optimize that has more than one variable?
            <br>
            We need multi-variable calculus, and that means we need to use partial derivatives $\frac{\partial f}{\partial x}$ to evaluate the tangent of the function at each input. If we want to solve the problem above $x^2+y^2$ with the constraints $xy=-16$ we can use <a href="http://www.math.harvard.edu/archive/21a_spring_09/PDF/11-08-Lagrange-Multipliers.pdf">Lagrange multipliers</a> to minimize or maximize the functions according to the constraint $\lambda$. The intuition for the Lagrange multipliers is best explained by this image:
        </p>
        <img src="felipunky.github.io/Images/Optimization/LagrangeMultipliers2D.svg" alt="LagrangeMultipliers">
        <p>
        The red curve shows the constraint $g(x, y) = c$. The blue curves are contours of $f(x, y)$. The point where the red constraint tangentially touches a blue contour is the maximum of $f(x, y)$ along the constraint, since $d1 > d2$.
        </p>
        <p>
        Let's solve the same problem $S(x,y)=x^2+y^2$ with constraint $g(x,y)=xy=-16$ with Lagrange multipliers
        <br>
        First we have to state the problem in terms of the Lagrangian $\mathcal{L}(x,y,\lambda)$, where $\lambda$ means that the gradient of $g(x,y)$ and $S(x,y)$ are proportional.
        $$\mathcal{L}(x,y,\lambda)=x^2+y^2-\lambda(xy-(-16))$$
        $$\mathcal{L}(x,y,\lambda)=x^2+y^2-{\lambda}x-{\lambda}y-{\lambda}16$$
        Now we need to find the partial derivatives of each of the inputs of our function $\mathcal{L}(x,y,\lambda)$ to find $\lambda$
        $$\frac{\partial \mathcal{L}}{\partial x}=2x-\lambda$$
        $$x=\dfrac{\lambda}{2}$$
        $$\frac{\partial \mathcal{L}}{\partial y}=2y-\lambda$$
        $$y=\dfrac{\lambda}{2}$$
        We know substitute our partial derivatives $\frac{\partial \mathcal{L}}{\partial x}$ and $\frac{\partial \mathcal{L}}{\partial y}$ into the constraint function $g(x,y)$
        $$g(x,y)=xy=-16$$
        $$\dfrac{\lambda}{2}^2=-16$$
        $$\dfrac{\lambda}{4}=-16$$
        $$\lambda^2=-64$$
        $$\lambda\pm\sqrt{64}$$
        $$\lambda\pm8$$
        Having solved $\lambda$ we can plug the value into $\frac{\partial \mathcal{L}}{\partial x}$
        $$\frac{\partial \mathcal{L}}{\partial x}=\dfrac{-8}{2}=-4\lor{\dfrac{8}{2}=4}$$
        And into $\frac{\partial \mathcal{L}}{\partial y}$
        $$\frac{\partial \mathcal{L}}{\partial y}=\dfrac{-8}{2}=-4\lor{\dfrac{8}{2}=4}$$
        We know we want to get $-16$ from $g(x,y)=xy$ so we have to choose either
        $$x=4$$
        $$y=-4$$
        $$\lor$$
        $$x=-4$$
        $$y=4$$
        We can also check with a higher order partial derivative, to see if we are $\cup$ or $\cap$
        $$\frac{\partial^2 \mathcal{L}}{\partial x^2}=2$$
        $$\frac{\partial^2 \mathcal{L}}{\partial y^2}=2$$
        As both second order partial derivatives are $\cup$ we are at a minimum.
        <br>
        We could also compute the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian</a> to test our result, but let's leave that as an excercise for the reader.
        <br>
        We can use this technique to optimize more inputs and even more constraints.
        </p>
        <p>
        To recap, we have done analytical (through algebraic manipulation) optimization. This is very handy if we have a problem that we have already approximated as a function and even a constraint that we also have approximated as a function. But there are cases in which we don't have a function that defines the problem, matter of fact it is generally easier to create a function that tests whether the answer is right or wrong (there are many names to this types of functions, fitness/cost function are some of them), this is actually the way that problems are ranked in computer science according to their <a href="https://en.wikipedia.org/wiki/Time_complexity#Complexity_classes">complexity classes</a>.
        </p>
        <h1>
            Numerical Solutions
        </h1>
        <p>
        Computers are excellent at iterating as they are fast at crunching numbers. So sometimes a numerical approach at optimization seems sound.
        <br>
        There are many techniques for iterative numerical optimization, but we are going to look at the following heuristics (a <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristic</a> (from Greek εὑρίσκω "I find, discover") is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution.) <a href="https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/">Gradient Descent</a>, <a href="http://www.mit.edu/~dbertsim/papers/Optimization/Simulated%20annealing.pdf">Simulated Annealing</a> and <a href="http://www.ra.cs.uni-tuebingen.de/mitarb/streiche/publications/Introduction_to_Evolutionary_Algorithms.pdf">Evolutionary Algorithms</a>.
        </p>
        <h2>
            Gradient Ascent/Descent
        </h2>
        <p>
        Gradient ascent/descent is an optimization algorithm that minimizes/maximizes the cost function of another algorithm by modifying the parameters of the given algorithm. It works by taking steps toward the positive/negative of the gradient to find local minima/maxima.
        <br>
        It is widely used to optimize the parameters of machine learning algorithms.
        <br>
        Let's visualize the algorithm on ${\displaystyle F(x,y)=\sin \left({\frac {1}{2}}x^{2}-{\frac {1}{4}}y^{2}+3\right)\cos \left(2x+1-e^{y}\right).}$
        <img src="felipunky.github.io/Images/Optimization/Gradient_ascent_(contour).png" alt="GradientAscentCountour">
        <img src="felipunky.github.io/Images/Optimization/Gradient_ascent_(surface).png" alt="GradientAscentSurface">
        <br>
        The problem with this heuristic is that the solution probably won't be the global minima/maxima. One variant of this algorithm, <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastich Gradient Descent or (SG)</a> can <a href="https://stats.stackexchange.com/questions/90874/how-can-stochastic-gradient-descent-avoid-the-problem-of-a-local-minimum">escape local maxima/minima</a> by the randomness introduced between "tracks" or iterations. However the "step size" (Learning Rate or (LR)) has to be tweaked according to the problem for full convergence.
        </p>
        <h2>
            Simulated Annealing
        </h2>
        <p>
        SG is actually very similar to Simulated Annealing or (SA), the difference is that instead of a "Learning Rate" or "Gamma", we refer to a "Temperature".
        <br>
        The temperature variable is inspired by the <a href="https://en.wikipedia.org/wiki/Annealing_(metallurgy)">process</a> used in metallurgy to heat into a melted state the material and then cool it slowly to increase the size of crystals while reducing the number of imperfections.
        <br>
        We use a probability function $A=\exp{ ( -\Delta f/kT)}$ to accept a state. Where $\Delta f$ is the rate of change from the energy of the previous iteration to the current one ($E_{current}-E_{previous}$), $k$ is the <a href="https://www.britannica.com/science/Boltzmann-constant">Boltzmann constant</a> and $T$ is the temperature (that is decreased every iteration, when this reaches the desired threshold it will also act as an exit condition for the iteration loop).
        <br>
        Here's a visualization of the process stolen from wikipedia
        <img src="felipunky.github.io/Images/Optimization/Hill_Climbing_with_Simulated_Annealing.gif" title="SimulatedAnnealingHillClimbing" />
        </p>
        <h2>
            Evolutionary Algorithms
        </h2>
        <p>
            There has been active research in genetic algorithms from the 1960's and they have been widely used by STEM professionals over the years to solve problems with many variables. Although there are a lot of implementations of Genetic Algorithms, we are going to use <a href="https://ieatbugsforbreakfast.wordpress.com/2011/03/04/epatps01/">David Rutten's</a> one as we are mostly concerned with CAD/CAE problems and the Galapagos solver is embedded in Grasshopper for Rhino3d, which is a great 3D tool that enables even non-programmers to program through visual scripting. Note that although Gradient Ascent/Descent is not implemented in Grasshopper, Simulated Annealing is in fact an extra solver in the Galapagos component.
        </p>
        <p>
        Rutten states that his solver consists of four mechanisms
        </p>
        <h3>
            Selection
        </h3>
        <p>
            When the genetic algorithm runs, it has to go somewhere from the results it has received.
            <br>
            Selection roughly means that from the Fitness (don't worry we will dive deeper into what a Fitness function means and how to create one) we are going to pick the best results to continue for the next round.
            <br>
            The term comes from the biological term Natural Selection, in which only the best offsprings will get the chance to reproduce.
        </p>
        <h3>
            Coupling
        </h3>
        <p>
        
        </p>
        <h3>
            Coalescence
        </h3>
        <p>
        
        </p>
        <h3>
            Mutation
        </h3>
        <p>
        
        </p>
    </div>
</body>
</html>
